# üé§ Voxtral + NLLB: Transcripci√≥n y Traducci√≥n en Tiempo Real

Sistema de transcripci√≥n de voz en tiempo real con traducci√≥n autom√°tica, dise√±ado para workshops de IA.

## üìã Descripci√≥n

Esta aplicaci√≥n combina dos modelos de IA:

| Modelo | Funci√≥n | Par√°metros | VRAM |
|--------|---------|------------|------|
| **Voxtral-Mini-4B-Realtime** | Transcripci√≥n en tiempo real | 4.4B | ~16GB |
| **NLLB-200-distilled-600M** | Traducci√≥n a ingl√©s | 600M | ~3GB |

### Caracter√≠sticas

- ‚úÖ Transcripci√≥n en tiempo real con latencia <500ms
- ‚úÖ Detecci√≥n autom√°tica de idioma
- ‚úÖ Soporte para 13 idiomas (transcripci√≥n)
- ‚úÖ Traducci√≥n a ingl√©s desde 200+ idiomas
- ‚úÖ Interfaz web con Gradio
- ‚úÖ API WebSocket compatible con vLLM Realtime

## üîß Requisitos de Hardware

| Componente | M√≠nimo | Recomendado |
|------------|--------|-------------|
| GPU | A10 24GB | A100 40GB/80GB |
| RAM | 16GB | 32GB |
| Disco | 50GB | 100GB (para cache) |

## üöÄ Despliegue en Verda Cloud

### Opci√≥n 1: Contenedor Unificado (Recomendado)

1. **Crear instancia en Verda:**
   - GPU: A100 40GB o superior
   - Imagen base: `vllm/vllm-openai:latest`
   - Puertos expuestos: `7860`, `8000`

2. **Clonar repositorio:**
   ```bash
   git clone https://github.com/acdonaire/transcriptor-voxtral-nllb.git
   cd transcriptor-voxtral-nllb
   ```

3. **Construir y ejecutar:**
   ```bash
   docker build -t voxtral-nllb:latest .
   docker run --gpus all -p 7860:7860 -p 8000:8000 voxtral-nllb:latest
   ```

### Opci√≥n 2: Docker Compose (dos contenedores)

```bash
docker-compose up -d
```

### Opci√≥n 3: Ejecuci√≥n directa en Verda

Si usas una instancia con vLLM preinstalado:

```bash
# Terminal 1: Iniciar vLLM
VLLM_DISABLE_COMPILE_CACHE=1 vllm serve mistralai/Voxtral-Mini-4B-Realtime-2602 \
    --host 0.0.0.0 \
    --port 8000 \
    --compilation_config '{"cudagraph_mode": "PIECEWISE"}' \
    --max-model-len 32768

# Terminal 2: Iniciar Gradio (despu√©s de que vLLM est√© listo)
pip install gradio transformers websockets soxr
python gradio-app/app.py
```

## üìÅ Estructura del Proyecto

```
voxtral-nllb-verda/
‚îú‚îÄ‚îÄ Dockerfile              # Imagen unificada
‚îú‚îÄ‚îÄ docker-compose.yml      # Orquestaci√≥n de 2 contenedores
‚îú‚îÄ‚îÄ supervisord.conf        # Gesti√≥n de procesos
‚îú‚îÄ‚îÄ start.sh               # Script de inicio manual
‚îú‚îÄ‚îÄ README.md              # Este archivo
‚îú‚îÄ‚îÄ vllm-voxtral/
‚îÇ   ‚îî‚îÄ‚îÄ Dockerfile         # Solo vLLM + Voxtral
‚îî‚îÄ‚îÄ gradio-app/
    ‚îú‚îÄ‚îÄ Dockerfile         # Solo Gradio + NLLB
    ‚îú‚îÄ‚îÄ app.py            # Aplicaci√≥n principal
    ‚îî‚îÄ‚îÄ requirements.txt   # Dependencias Python
```

## üåê Acceso a la Aplicaci√≥n

Una vez desplegado:

- **Interfaz Gradio**: `http://<IP_VERDA>:7860`
- **API vLLM**: `http://<IP_VERDA>:8000`
- **WebSocket Realtime**: `ws://<IP_VERDA>:8000/v1/realtime`

## üìñ Uso

1. Abre la interfaz Gradio en tu navegador
2. Haz clic en **üé§ Iniciar**
3. Permite el acceso al micr√≥fono
4. Habla en cualquier idioma soportado
5. La transcripci√≥n aparece en tiempo real
6. La traducci√≥n al ingl√©s se genera autom√°ticamente
7. Haz clic en **‚èπÔ∏è Detener** cuando termines

## üåç Idiomas Soportados

### Transcripci√≥n (Voxtral)
Espa√±ol, Ingl√©s, Franc√©s, Alem√°n, Italiano, Portugu√©s, Holand√©s, Ruso, Chino, Japon√©s, Coreano, √Årabe, Hindi

### Traducci√≥n (NLLB)
200+ idiomas incluyendo todos los anteriores y muchos m√°s

## ‚öôÔ∏è Configuraci√≥n Avanzada

### Variables de Entorno

| Variable | Descripci√≥n | Default |
|----------|-------------|---------|
| `VLLM_HOST` | Host del servidor vLLM | `localhost` |
| `VLLM_PORT` | Puerto del servidor vLLM | `8000` |
| `GRADIO_PORT` | Puerto de la interfaz Gradio | `7860` |
| `HF_TOKEN` | Token de HuggingFace (opcional) | - |

### Ajustes de VRAM

Para GPUs con menos memoria, ajusta `--gpu-memory-utilization`:

```bash
# Para A10 24GB
--gpu-memory-utilization 0.85

# Para A100 40GB
--gpu-memory-utilization 0.70

# Para A100 80GB
--gpu-memory-utilization 0.50
```

### Ajuste de Latencia

El delay de transcripci√≥n se puede configurar (480ms es el sweet spot):

```bash
# En params.json del modelo
"transcription_delay_ms": 480  # 80ms a 2400ms
```

## üêõ Soluci√≥n de Problemas

### vLLM no inicia
```bash
# Verificar GPU disponible
nvidia-smi

# Probar con modo eager
vllm serve ... --enforce-eager
```

### WebSocket no conecta
```bash
# Verificar que vLLM est√© corriendo
curl http://localhost:8000/health

# Ver logs
docker logs vllm-voxtral
```

### Error de VRAM
```bash
# Reducir utilizaci√≥n de memoria
--gpu-memory-utilization 0.5

# Reducir contexto m√°ximo
--max-model-len 16384
```

## üìú Licencias

- **Voxtral-Mini-4B**: Apache 2.0 ‚úÖ
- **NLLB-200**: CC-BY-NC-4.0 ‚ö†Ô∏è (solo uso no comercial)

## üîó Referencias

- [Voxtral Model Card](https://huggingface.co/mistralai/Voxtral-Mini-4B-Realtime-2602)
- [NLLB-200 Model Card](https://huggingface.co/facebook/nllb-200-distilled-600M)
- [vLLM Realtime API](https://docs.vllm.ai/en/latest/serving/openai_compatible_server/)
- [Verda Cloud](https://verda.com)

---

**ColoqIALab** - Workshop de IA | Febrero 2026
